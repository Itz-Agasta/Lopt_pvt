{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scarlet.ipynb\n",
    "\n",
    "# Deepfake Detection using TimeSformer and FaceForensics++ (C23) on Kaggle\n",
    "\n",
    "# --------------------\n",
    "# 1. INSTALL DEPENDENCIES\n",
    "# --------------------\n",
    "# Install essential libraries for model training and video processing\n",
    "!pip install -q transformers accelerate timm decord --no-deps\n",
    "!pip install decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 2. IMPORT LIBRARIES\n",
    "# --------------------\n",
    "import os  # For interacting with the file system\n",
    "import random  # For random sampling and shuffling\n",
    "import json  # For saving evaluation metrics\n",
    "import torch  # PyTorch framework\n",
    "from torch import nn  # Neural network modules\n",
    "from decord import VideoReader, cpu\n",
    "from torch.utils.data import Dataset, DataLoader  # Dataset and data loading utilities\n",
    "import torchvision.transforms as T  # Image transformations\n",
    "import cv2  # OpenCV for image and video operations\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification  # Hugging Face model and processor\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix  # Evaluation metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 3. SETUP DEVICE\n",
    "# --------------------\n",
    "# Use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150af40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 4. CONFIGURATION\n",
    "# --------------------\n",
    "\n",
    "# Install dependencies as needed:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Update these paths after checking them in the file explorer\n",
    "df = pd.read_csv(\"/kaggle/input/ff-c23/FaceForensics++_C23/csv/original.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/input/ff-c23/FaceForensics++_C23/csv/Deepfakes.csv\")\n",
    "\n",
    "print(\"First 5 records (real):\", df.head())\n",
    "print(\"First 5 records (fake):\", df1.head())\n",
    "\n",
    "\n",
    "real_path = \"/kaggle/input/ff-c23/FaceForensics++_C23/original\"\n",
    "fake_path = \"/kaggle/input/ff-c23/FaceForensics++_C23/Deepfakes\"\n",
    "\n",
    "# Video processing and training parameters\n",
    "num_frames = 8  # Number of frames to sample from each video\n",
    "\n",
    "''' Why have we chosen frames to be 8:-\n",
    "\n",
    "TimeSformer models require a fixed number of frames from each video input.\n",
    "Using 8 frames provides a balance between temporal representation and computational efficiency.\n",
    "Fewer frames (e.g., 4) might not capture enough temporal context to distinguish real from fake.\n",
    "More frames (e.g., 16 or 32) increase memory usage significantly, which can cause OOM (Out of Memory) errors on limited GPUs like those on Kaggle.'''\n",
    "\n",
    "image_size = 224  # Target image size (height, width)\n",
    "\n",
    "''' Why have we chosen image size as 224x224:- \n",
    "\n",
    "224x224 is the standard input size for many pretrained image and video models including TimeSformer.\n",
    "Pretrained weights from \"facebook/timesformer-base-finetuned-k400\" expect 224x224 resolution.\n",
    "Resizing videos to this shape keeps consistency with the model’s original training distribution.'''\n",
    "\n",
    "batch_size = 2  # Number of samples per batch\n",
    "\n",
    "''' Why have we chosen batch size to be 2:- \n",
    "\n",
    "Video models are extremely memory-intensive since they process multiple frames per sample.\n",
    "On Kaggle’s free GPU (often a T4 with 16GB), a batch size of 2 is safe and avoids OOM errors.\n",
    "We can experiment with higher batch sizes (e.g., 4 or 8) if we reduce the number of frames or image size.'''\n",
    "\n",
    "epochs = 21 # Total number of training epochs\n",
    "\n",
    "''' Why have we chosen epochs to be :- \n",
    "\n",
    "Performance Balance: 21 epochs capture sufficient learning before overfitting starts.\n",
    "Resource Considerations: Limited time and GPU resources often make 21 a good upper bound.\n",
    "Validation Results: 21 epochs may provide the best validation performance observed during training.'''\n",
    "\n",
    "checkpoint_path = \"/kaggle/working/timesformer_ffpp_checkpoint.pth\"  # Path to save/resume checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 5. TRANSFORMATIONS\n",
    "# --------------------\n",
    "# Define transformation for each frame in the video\n",
    "transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),  # Resize frames\n",
    "    T.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    T.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef66f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 6. DATASET CLASS\n",
    "# --------------------\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for loading videos and converting them into frame tensors.\n",
    "\n",
    "    Args:\n",
    "        video_paths (List[str]): Paths to video files.\n",
    "        labels (List[int]): Binary labels corresponding to real (0) or fake (1) videos.\n",
    "        transform (Callable): Transformation function applied to each frame.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_paths, labels, transform):\n",
    "        self.video_paths = video_paths  # Store paths to video files\n",
    "        self.labels = labels  # Store associated labels\n",
    "        self.transform = transform  # Store frame transformation function\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and processes a video at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            video (Tensor): Tensor of shape [C, T, H, W].\n",
    "            label (int): Class label.\n",
    "        \"\"\"\n",
    "        path = self.video_paths[idx]  # Get the file path\n",
    "        label = self.labels[idx]  # Get the label\n",
    "        vr = VideoReader(path, ctx=cpu(0))  # Load video\n",
    "        total_frames = len(vr)  # Total number of frames\n",
    "        indices = [int(i * total_frames / num_frames) for i in range(num_frames)]  # Sample evenly spaced frames\n",
    "        frames = [self.transform(Image.fromarray(vr[i].asnumpy())) for i in indices] # Read and transform frames\n",
    "        video = torch.stack(frames)  # T, C, H, W\n",
    "        video = video.permute(0, 1, 2, 3)  # (unchanged, safe way to keep T, C, H, W)\n",
    "        return video, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c278cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 7. LOAD AND BALANCE DATA\n",
    "# --------------------\n",
    "# Get video file names\n",
    "real_videos = [os.path.join(real_path, f) for f in os.listdir(real_path) if f.endswith(\".mp4\")]\n",
    "fake_videos = [os.path.join(fake_path, f) for f in os.listdir(fake_path) if f.endswith(\".mp4\")]\n",
    "\n",
    "# Sample an equal number of real and fake videos\n",
    "sample_size = min(len(real_videos), len(fake_videos), 350)\n",
    "real_videos = random.sample(real_videos, sample_size)\n",
    "fake_videos = random.sample(fake_videos, sample_size)\n",
    "\n",
    "# Combine and shuffle data\n",
    "video_paths = real_videos + fake_videos\n",
    "labels = [0] * sample_size + [1] * sample_size\n",
    "combined = list(zip(video_paths, labels))\n",
    "random.shuffle(combined)\n",
    "video_paths, labels = zip(*combined)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_paths = video_paths[:600]\n",
    "train_labels = labels[:600]\n",
    "val_paths = video_paths[600:700]\n",
    "val_labels = labels[600:700]\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = VideoDataset(train_paths, train_labels, transform)\n",
    "val_dataset = VideoDataset(val_paths, val_labels, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf420161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 8. LOAD MODEL\n",
    "# --------------------\n",
    "from transformers import TimesformerForVideoClassification\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the pretrained model as-is (400 output classes)\n",
    "model = TimesformerForVideoClassification.from_pretrained(\n",
    "    \"facebook/timesformer-base-finetuned-k400\"\n",
    ")\n",
    "\n",
    "# Step 2: Replace classifier BEFORE loading checkpoint\n",
    "if hasattr(model, 'classifier'):\n",
    "    model.classifier = nn.Linear(model.config.hidden_size, 2)\n",
    "    print(\"✅ Replaced classifier layer with binary classification head.\")\n",
    "elif hasattr(model, 'head'):\n",
    "    model.head = nn.Linear(model.config.hidden_size, 2)\n",
    "    print(\"✅ Replaced head layer with binary classification head.\")\n",
    "else:\n",
    "    print(\"⚠️ Unable to find classification head ('classifier' or 'head').\")\n",
    "\n",
    "# Step 3: Load best model weights (after classifier has been replaced)\n",
    "best_model_path = \"/kaggle/working/outputs/Scarlet.pt\"  # Your best model path\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device, weights_only=True))\n",
    "    print(\"✅ Loaded best model from Scarlet.pt\")\n",
    "\n",
    "# Step 4: Move model to device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a945bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 9. TRAINING SETUP\n",
    "# --------------------\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-2)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create output directory for saving results\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Initialize best accuracy for model saving\n",
    "best_val_acc = 0.0\n",
    "metrics_dict = {}  # Dictionary to store evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8027b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 10. RESUME CHECKPOINT (if exists)\n",
    "# --------------------\n",
    "# Variables to resume training\n",
    "start_epoch = 0\n",
    "start_batch_idx = 0\n",
    "\n",
    "# If checkpoint exists, resume from it\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_batch_idx = checkpoint['batch_idx'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch + 1}, batch {start_batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 11. TRAINING LOOP\n",
    "# --------------------\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0  # Track total loss\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Skip already trained batches when resuming\n",
    "        if epoch == start_epoch and batch_idx < start_batch_idx:\n",
    "            continue\n",
    "\n",
    "        videos, labels = batch  # Get a batch of videos and labels\n",
    "        videos = videos.to(device)  # shape: (B, T, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(pixel_values=videos).logits  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Save checkpoint every 20 batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'batch_idx': batch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}, batch {batch_idx}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # --------------------\n",
    "    # VALIDATION LOOP\n",
    "    # --------------------\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation\n",
    "        for batch in val_loader:\n",
    "            videos, labels = batch\n",
    "            videos = videos.to(device)  # shape: (B, T, C, H, W)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=videos).logits  # Forward pass\n",
    "            preds = torch.argmax(outputs, dim=1)  # Predicted classes\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = correct / total  # Accuracy\n",
    "    val_f1 = f1_score(all_labels, all_preds)  # F1 Score\n",
    "    val_auc = roc_auc_score(all_labels, all_preds)  # AUC Score\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/kaggle/working/outputs/confusion_matrix.png\")  # Save figure\n",
    "    plt.show()\n",
    "\n",
    "    # Save classification report and confusion matrix\n",
    "    with open(\"/kaggle/working/outputs/classification_report.txt\", \"w\") as f:\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\n",
    "        f.write(\"\\nConfusion Matrix:\\n\")\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        f.write(str(cm))\n",
    "\n",
    "\n",
    "    # Save best model and metrics\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/outputs/Scarlet.pt\")  # Save model\n",
    "        metrics_dict = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"accuracy\": val_acc,\n",
    "            \"f1_score\": val_f1,\n",
    "            \"auc\": val_auc\n",
    "        }\n",
    "        with open(\"/kaggle/working/outputs/metrics.json\", \"w\") as f:\n",
    "            json.dump(metrics_dict, f, indent=4)\n",
    "        print(\" Best model and metrics saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172abacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 12. SAVE FINAL MODEL\n",
    "# --------------------\n",
    "torch.save(model.state_dict(), \"/kaggle/working/timesformer_ffpp_final.pth\")  # Save final model\n",
    "print(\" Final model saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Virtus - Deepfake Image Detection Model\n",
    "\n",
    "In this notebook, we fine-tune a Vision Transformer (ViT) model for detecting deepfakes in images.\n"
   ],
   "id": "a3466953896a728"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc  # Garbage collection interface\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Data handling & plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Oversampling for imbalanced datasets\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Hugging Face libraries\n",
    "import accelerate\n",
    "import evaluate\n",
    "from datasets import Dataset, Image, ClassLabel\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTImageProcessor,\n",
    "    ViTForImageClassification,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Torchvision transforms for data augmentation\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    CenterCrop,\n",
    "    Normalize,\n",
    "    RandomRotation,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize,\n",
    "    ToTensor\n",
    ")\n",
    "\n",
    "# PIL configuration to handle corrupted images gracefully\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Allows PIL to load partially corrupted images"
   ],
   "id": "5dc07d9e6181a719"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Preparation & Preprocessing\n",
    "\n",
    "This section outlines the complete dataset preprocessing pipeline. It involves loading image paths and corresponding labels from the directory structure, applying `RandomOverSampler` to balance the dataset, and creating a Hugging Face `Dataset` object. The pipeline also includes the mapping of string labels to integer class IDs, followed by splitting the dataset into training and testing sets with a 60:40 ratio, ensuring stratification for balanced class distribution.\n",
    "\n"
   ],
   "id": "d81e5e4f9ac4729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reference: https://huggingface.co/docs/datasets/en/image_load\n",
    "image_dict = {}\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_names = []\n",
    "labels = []\n",
    "\n",
    "# Define the path to your dataset\n",
    "data_path = Path('xyz')\n",
    "\n",
    "# Iterate through all files in the dataset directory (assumes structure: root/class_name/image.jpg)\n",
    "for file in tqdm(sorted(data_path.glob('*/*/*.*'))):\n",
    "    label = file.parts[-2]  # Get the second-last part as the label\n",
    "    labels.append(label)\n",
    "    file_names.append(str(file))\n",
    "\n",
    "# Check length consistency with total number of files & labels\n",
    "print(len(file_names), len(labels))\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\n",
    "print(df.shape)"
   ],
   "id": "458422ec68373c89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head() # Preview the DataFrame",
   "id": "aabbaf3bb688ce56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['label'].unique() # View unique class labels",
   "id": "738e75f1cd4866cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random resampling of minority class to balance the dataset\n",
    "y = df[['label']]\n",
    "df = df.drop(['label'], axis=1)\n",
    "ros = RandomOverSampler(random_state=83)\n",
    "df, y_resampled = ros.fit_resample(df, y)\n",
    "\n",
    "# Clean up temporary variables\n",
    "del y\n",
    "df['label'] = y_resampled\n",
    "del y_resampled\n",
    "gc.collect()\n",
    "\n",
    "print(df.shape)"
   ],
   "id": "42dcab83f9d2fb14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Hugging Face Dataset from a pandas DataFrame\n",
    "# This is useful when transitioning from tabular data (DataFrame) to a Dataset\n",
    "# for preprocessing or training with Hugging Face tools\n",
    "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())"
   ],
   "id": "27e0bfee9bfa04f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset[0][\"image\"] # Display the first image in the dataset (as a PIL.Image)",
   "id": "72894a2649a20604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract a subset of examples (first 5) to inspect structure or labels\n",
    "labels_subset = dataset[:5]\n",
    "print(labels_subset)"
   ],
   "id": "bf21e35ac4951b43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the list of unique labels (ensure consistency with training labels)\n",
    "# In this dataset, 0 = Real, 1 = Fake\n",
    "labels_list = ['Real', 'Fake']\n",
    "\n",
    "# Create label â†” ID mapping dictionaries\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels_list):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "print(f\"Mapping of IDs to labels:{label2id}\\n\")\n",
    "print(f\"Mapping of labels to IDs:{id2label}\")"
   ],
   "id": "22c5597d71e351ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a ClassLabel object to match string labels to integer IDs\n",
    "class_labels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
    "\n",
    "# Mapping string labels to their corresponding IDs\n",
    "def map_label2id(example):\n",
    "    example[\"label\"] = class_labels.str2int(example[\"label\"])\n",
    "    return example\n",
    "\n",
    "# Hugging Face Dataset expects the label column to be of type ClassLabel\n",
    "dataset = dataset.map(map_label2id, batched=True)\n",
    "dataset = dataset.cast_column('label', ClassLabel)\n",
    "\n",
    "# Split the dataset into training and testing sets (60:40 split), stratified by label\n",
    "dataset = dataset.train_test_split(test_size=0.4, shuffle=True, stratify_by_column=\"label\")\n",
    "\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "#--Dataset processing is done--#"
   ],
   "id": "bc0adc7052f3de75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Loading and Training Pipeline\n",
    "\n",
    "This section covers the process of loading the pre-trained Vision Transformer (ViT) model, configuring the necessary training parameters, and fine-tuning the model on the prepared dataset.\n",
    "\n",
    "\n"
   ],
   "id": "1b5140efa2d074ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_str = \"xyz\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_str)\n",
    "\n",
    "# Extract normalization parameters used during model pretraining\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "# Target image size for the model\n",
    "size = processor.size[\"height\"]\n",
    "print(\"Resize target size:\", size)\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std) # To make the model unbiased\n",
    "\n",
    "# Define transformations for training data\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "    Resize((size, size)),\n",
    "    RandomRotation(90),             # Adds rotation-based augmentation\n",
    "    RandomAdjustSharpness(2),       # Enhances sharpness to simulate noise variations\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define transformations for validation data (no augmentation)\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "    Resize((size, size)),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    \"\"\"\n",
    "    Apply training transformations to a batch of examples.\n",
    "    Converts each image to RGB and applies augmentation.\n",
    "    \"\"\"\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    \"\"\"\n",
    "    Apply validation transformations to a batch of examples.\n",
    "    Converts each image to RGB and applies resizing & normalization.\n",
    "    \"\"\"\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ],
   "id": "d27940029e66415d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set custom transform functions for the training and validation datasets\n",
    "train_data.set_transform(train_transforms)\n",
    "test_data.set_transform(val_transforms)"
   ],
   "id": "a28e147adb5829c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Custom collate function to batch input images and labels for training.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - 'pixel_values': Tensor of stacked image tensors\n",
    "            - 'labels': Tensor of corresponding class labels\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "id": "4d0ba1dc1a3f179e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load pre-trained Vision Transformer model with the correct number of labels\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_str,\n",
    "    num_labels=len(labels_list)\n",
    ")\n",
    "\n",
    "# Map class labels to IDs and vice versa\n",
    "model.config.label2id = label2id\n",
    "model.config.id2label = id2label\n",
    "\n",
    "print(f\"Trainable Parameters: {model.num_parameters(only_trainable=True) / 1e6:.2f}M\")"
   ],
   "id": "62dc1e9122a22127"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (EvalPrediction): A namedtuple with 'predictions' and 'label_ids'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with accuracy score.\n",
    "    \"\"\"\n",
    "    predictions = eval_pred.predictions\n",
    "    label_ids = eval_pred.label_ids\n",
    "    predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n",
    "    return {\"accuracy\": acc_score}"
   ],
   "id": "636a838c43501e86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define training hyperparameters and configuration\n",
    "metric_name = \"accuracy\"\n",
    "model_name = \"virtus\"\n",
    "num_train_epochs = 2\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_name,               # Directory to save model checkpoints\n",
    "    logging_dir='./logs',                # Directory to save logs\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate model at the end of each epoch\n",
    "    learning_rate=1e-6,                  # Low learning rate for stable fine-tuning\n",
    "    per_device_train_batch_size=32,      # Batch size for training\n",
    "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
    "    num_train_epochs=num_train_epochs,   # Number of training epochs\n",
    "    weight_decay=0.02,                   # Helps prevent overfitting\n",
    "    warmup_steps=50,                     # Warm-up steps for learning rate scheduler\n",
    "    remove_unused_columns=False,         # Retain all columns (e.g., pixel_values)\n",
    "    save_strategy=\"epoch\",               # Save checkpoint every epoch\n",
    "    load_best_model_at_end=True,         # Restore best model based on eval metric\n",
    "    save_total_limit=1,                  # Keep only the best checkpoint to save disk space\n",
    "    report_to=\"none\"                     # Disable reporting (e.g., to WandB)\n",
    ")"
   ],
   "id": "49cbec527808b3f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a Trainer instance for fine-tuning the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ],
   "id": "aa3ee186ccc941f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let the game begin\n",
    "trainer.train()"
   ],
   "id": "64ed50233b44da7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the Post-training model's performance on the validation / test dataset\n",
    "# Returns final accuracy and other defined metrics\n",
    "trainer.evaluate()"
   ],
   "id": "8df4af9b28b40ba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Classification Report & Confusion Matrix\n",
    "\n",
    "Visualize and print detailed metrics including accuracy, F1 score, and a confusion matrix.\n"
   ],
   "id": "ace7c0cc52a53661"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Perform predictions on the test dataset\n",
    "outputs = trainer.predict(test_data)\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(outputs.metrics)"
   ],
   "id": "c4abc3a894001035"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preview predicted vs actual labels for the first few samples\n",
    "preds = outputs.predictions.argmax(axis=1)\n",
    "labels = outputs.label_ids\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\" Predicted: {id2label[preds[i]]} | Actual: {id2label[labels[i]]}\")"
   ],
   "id": "4da1934ac2e085d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(axis=1)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix with labels and color-coded heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.0f'\n",
    "    thresh = cm.max() / 2.0\n",
    "\n",
    "    # Add number labels inside heatmap\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), ha=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and print classification metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\" F1 Score (macro): {f1:.4f}\")\n",
    "\n",
    "# Generate confusion matrix and classification report\n",
    "if len(labels_list) <= 150:  # Avoid crashing on huge class sets\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cm, classes=labels_list, figsize=(8, 6))\n",
    "\n",
    "    print(\"\\nðŸ§¾ Classification Report:\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))\n"
   ],
   "id": "a7f3632ba637c7f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.save_model()",
   "id": "e81cfac9337b74fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (virtus)",
   "language": "python",
   "name": "virtus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
